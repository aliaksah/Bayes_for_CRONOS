---
title: "Proportion of literacy deprived students per school"
output:
  word_document: default
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
---
### Load R packages
```{r}
source("RFiles/packages.r")
```

### Load  single imputed PISA data.
```{r}
#load(file = file.path("Orgdata/IMPDAT_PISA.RData"))
```

### Prepare data, thus build PISA scales, 6489 students 
```{r}
#source("RFiles/01data_preperation.r")
#to.rm <- objects()[!objects() %in% c("PPath","pisa18","countries")]
#rm(list = to.rm)
# head(pisa18)
# dim(pisa18)
```


### There should be at least ten students per school. Schools with fewer observations are excluded from the analysis. The variable nb.stud informs about the number of stundets per school and save final data set

```{r}
#tab.id <- table(pisa18$school.id)
#id.frame <- as.data.frame(tab.id)

#nb.stud <- rep(id.frame$Freq,id.frame$Freq)
#pisa18$nb.stud <- nb.stud
#rm(tab.id,id.frame,nb.stud)
#pisa.data <- subset(pisa18, pisa18$nb.stud > 9)
#pisa18 <- pisa.data

#rm(pisa.data)
#summary(pisa18$nb.stud)
#save(pisa18,file = "Files/pisa18.RData")
load("Files/pisa18.RData")
```



### Data sets for schools: Data set contains one row for each school and only school level variables
```{r}
#sc.level <- names(pisa18) == toupper(names(pisa18))
#sc.level[1] <- TRUE
#pisa18_sc <- pisa18[,sc.level]
#dim(pisa18_sc)
#pisa18_sc <- pisa18_sc[!duplicated(pisa18_sc$school.id), ]
#dim(pisa18_sc)
#names(pisa18_sc)
#save(pisa18_sc,file = "Files/pisa18_sc.RData")
#load( "Files/pisa18_sc.RData")
```
### Cut focal cont. predictors in qurtiles, use later in PPC to ckeck model validity for certain regions of the outcome distribution
```{r}
pisa18$DH_QU <- as.factor(cut_number(pisa18$DISHOME,4))
levels_DH <- levels(pisa18$DH_QU)
ds_DH <- pisa18 %>% group_by(pisa18$DH_QU) %>% summarise(median = median(pisa18$DISHOME), Freq = n())  
levels(pisa18$DH_QU) <- 1:4

pisa18$BL_QU <- as.factor(cut_number(pisa18$BELONG,4))
levels_BL <- levels(pisa18$BL_QU)
ds_BL <- pisa18 %>% group_by(pisa18$BL_QU) %>% summarise(median = median(pisa18$BELONG), Freq = n())  
levels(pisa18$BL_QU) <- 1:4

pisa18$ATT_QU <- as.factor(cut_number(pisa18$ATT,4))
levels_ATT <- levels(pisa18$ATT_QU)
ds_ATT <- pisa18 %>% group_by(pisa18$ATT_QU) %>% summarise(median = median(pisa18$ATT), Freq = n())  
levels(pisa18$ATT_QU) <- 1:4

pisa18$RES_QU <- as.factor(cut_number(pisa18$RES,4))
levels_RES <- levels(pisa18$RES_QU)
ds_RES <- pisa18 %>% group_by(pisa18$RES_QU) %>% summarise(median = median(pisa18$RES), Freq = n())  
levels(pisa18$RES_QU) <- 1:4
```
### Stack models: Use all relevant predictors, hold out some to check for predictive validity
### Splines: https://rpubs.com/samuelfix08/splines
```{r}
knots <- quantile(pisa18$DISHOME, p = c(0.25, 0.5, 0.75))
#model <- lm (medv ~ bs(lstat, knots = knots), data = train.data)
# ESCS TEACHBAD BULL
# core model
corem <- as.formula(ld ~ STAFFLACK + bs(DISHOME, knots = knots) + (1 | school.id))# 2,4

# models to stack
# reading attitudes and learning attitudes + FEM
stack1 <- update(corem, ~. + comp + diff  + joyread + teachsup +
                       mot +  goal + val +RES)# 8

# difficulties, lack of resources
stack2 <- update(corem,~ . + immig + fewbooks  +  bull + escs  + repeatclass + parent_sup +
                        ATT)# 7


# intrinsic to school level -> school climate and classroom discipline
stack3 <- update(corem,~ . +  disclima + 
                   perfeed + dirins + adaptive + enthus +fof + BELONG ) # 7


# other relevant, and all key school in one model
stack4 <- update(corem,~ .  +  female  + native + BELONG + ATT + RES  + UNI) # 6

stack5 <- update(corem,~ .  +  FEMFRAC + NATIVE + belong + att + res  + uni) # 6


models <- vector(mode = "list", length = 3)

models[[1]] <- stack1
models[[2]] <- stack2
models[[3]] <- stack3
models[[4]] <- stack4
models[[5]] <- stack5


```

### Estimate models with stan_glmer, 
### aim: flexible shape and low variance of the posterior estimation see e. g. https://projecteuclid.org/journals/bayesian-analysis/volume-13/issue-3/Using-Stacking-to-Average-Bayesian-Predictive-Distributions-with-Discussion/10.1214/17-BA1091.full?tab=ArticleLinkSupplemental

```{r}
a.seed <- 12345 
a.iter <-  2000 
a.chains <- 4  

fits <- list()

fits[[1]] <- stan_glmer(models[[1]], data = pisa18, seed = a.seed,
                      iter = a.iter, chains = a.chains,family = binomial()) 

fits[[2]] <- stan_glmer(models[[2]], data = pisa18, seed = a.seed,
                      iter = a.iter, chains = a.chains,family = binomial()) 

fits[[3]] <- stan_glmer(models[[3]], data = pisa18, seed = a.seed,
                      iter = a.iter, chains = a.chains,family = binomial())

fits[[4]] <- stan_glmer(models[[4]], data = pisa18, seed = a.seed,
                      iter = a.iter, chains = a.chains,family = binomial())

fits[[5]] <- stan_glmer(models[[5]], data = pisa18, seed = a.seed,
                      iter = a.iter, chains = a.chains,family = binomial())


save(fits, file = file.path("Files/fits.RData"))
```


###load fits

```{r}
load(file.path("Files/fits.RData"))
```


### Model parameters

```{r}
n01.models <- length(models)
sink(file = file.path("Results/fixed_par.txt"))
for (i in 1:n01.models) {
  print(kable(tidy(fits[[i]],conf.int  = T, conf.level = .95, 
                   effects = "fixed"),"simple",digits = 4, 
              caption = paste("model ",i," *****")))
}
sink(file = NULL)

sink(file = file.path("Results/ran_par.txt"))
for (i in 1:n01.models){
  print(kable(tidy(fits[[i]],conf.int=T, conf.level=.95, effects = "ran_pars"),"simple",digits=4, caption = paste("model ",i," *****")))
  
  }
sink(file = NULL)

```

### Intraclass correlation
### https://lnalborczyk.github.io/post/icc/

### Loo list and stacking weights

```{r}
loo_list <- list()
# Compute loo

loo_list <- lapply(fits, loo, cores = 1)
# Save loo

save("loo_list", file = file.path("Files","loo_list.RData"))
#load(file.path("Files","loo_list.RData"))

sink(file = file.path("Files","loo_info.txt"))
for (i in 1:n01.models) {
  print(paste("******** "))
  print(paste("model ", i))
  print(loo_list[[i]])
  print(paste("******** "))
}
#print(loo_list[[1]])
sink(file = NULL)

wtsStacking <- list()

wtsStacking <- loo_model_weights(loo_list, method = "stacking")
print(wtsStacking)


sink(file = file.path("Results","wtsStacking.txt"))
print(wtsStacking)
print(paste("******** "))
sink(file = NULL)

wtsPseudoBMA <- loo_model_weights(loo_list, method = "pseudobma")
print(wtsPseudoBMA)


```
### Load functions
```{r}
source("RFiles/02functions.r")
```
### Compute Brier Score
```{r}
set.seed(a.seed)
ypredStack <- ypred_Stacking(fits,loo_list, method = "stacking") 
save("ypredStack", file = file.path("Files/ypredStack.RData"))
load("Files/ypredStack.RData")


ypredCand <- list()
for (i in 1:n01.models) {
  set.seed(a.seed)
  ypredCand[[i]] <- posterior_predict(fits[[i]])
}
save("ypredCand", file = file.path("Files/ypredCand.RData"))
#load("Files/ypredCand.RData")


## Brier Score 

dim(ypredStack)
br.stackL2 <- brierS(pisa18$ld,colMeans(ypredStack))

br.candL2 <- list()
for (i in 1:n01.models) {
  br.candL2[[i]] <- brierS(pisa18$ld,colMeans(ypredCand[[i]]))
 
}

# Write Brier score to file.
sink(file = file.path("Results/brier.scores.txt"))

print("stack *******")
print(br.stackL2)


for (i in 1:n01.models) {
  print(paste("model ", i))
  print(br.candL2[[i]])
  
}
sink(file = NULL)
```
### Posterior predictive checks
```{r}
# hold-out predictor
pisa18$WITHOUT  <- as.numeric(pisa18$FAILED >= 0.02) # above median

# For each quartile compute the median sumDISHOME.
sumDISHOME <- pisa18 %>%
  group_by(DH_QU) %>%
  summarise(mDISHOME = median(DISHOME), sd = sd(DISHOME),min = min(DISHOME),max = max(DISHOME))

medDISHOME <- sumDISHOME$mDISHOME


prev8 <- pisa18 %>%
  group_by(STAFFLACK,DH_QU) %>%
  summarise(mLD = mean(ld))


sink(file = file.path("Results/prev.txt"))
print("Prev")
round(unlist(prev8$mLD),3)
sink(file = NULL)
#  

# https://mc-stan.org/bayesplot/reference/available_ppc.html


source(file = file.path("RFiles", "pppv.r"))

#color_scheme_set(scheme = "purple")



#color_scheme_set(scheme = "mix-teal-pink") # "brewer-Spectral"
color_scheme_set(scheme = "gray")
ppcStack <- visualPPC()#y.obs = pisa18$ld, yrepM = ypredStack
pdf(file.path("Results/ppcStack.pdf"))
ppcStack
dev.off()



save.image(file = file.path("Files/save_session.RData"))
#load(file=file.path(cnt.path,"save_session.RData"))

```
### Compute results DH_QU
```{r}
sgr <- split(pisa18,f = list(pisa18$DH_QU,pisa18$STAFFLACK))
t<-lapply(sgr, function(x) length(x[[1]]))
pint <- comp.int.stack.core01(sgr = sgr,fits = fits,
                                     wts = wtsStacking,
                                     cnt.path = "Files", p = 0.5)
```
### Summarize results DH_QU
```{r}
dfDH_QU <- data.frame(x = rep(round(medDISHOME,2),2),
                        group = c(rep("no",4),rep("yes",4)),
                        mean =  pint[,2],low = pint[,1], 
                        up = pint[,3])
sink(file = file.path("Results/dfDH_QU.txt"))
dfDH_QU
sink(file = NULL)
```

### Plot results
```{r}
pdf(file.path("Results/dfDH_QU.pdf"))
plot.result(dfDH_QU)
dev.off()
```
### Compute results BL_QU
```{r}
sgrBL <- split(pisa18,f = list(pisa18$BL_QU,pisa18$STAFFLACK))
#t<-lapply(sgrBL, function(x) length(x[[1]]))
pintBL <- comp.int.stack.core01(sgr = sgrBL,fits = fits,
                                     wts = wtsStacking,
                                     cnt.path = "Files", p = 0.5)
```
### Summarize results BL_QU

```{r}

# For each quartile compute the median sumBELONG.
sumBL <- pisa18 %>%
  group_by(BL_QU) %>%
  summarise(mBL = median(BELONG), sd = sd(BELONG),min = min(BELONG),max = max(BELONG))

medBL <- sumBL$mBL

dfBL_QU <- data.frame(x = rep(round(medBL,2),2),
                        group = c(rep("no",4),rep("yes",4)),
                        mean =  pintBL[,2],low = pintBL[,1], 
                        up = pintBL[,3])
sink(file = file.path("Results/dfBL_QU.txt"))
dfBL_QU
sink(file = NULL)
```


```{r}
pdf(file.path("Results/dfBL_QU.pdf"))
plot.result(dfBL_QU)
dev.off()
```
### Compute results ATT_QU
```{r}
sgrATT <- split(pisa18,f = list(pisa18$ATT_QU,pisa18$STAFFLACK))
#t<-lapply(sgrBL, function(x) length(x[[1]]))
pintATT <- comp.int.stack.core01(sgr = sgrATT,fits = fits,
                                     wts = wtsStacking,
                                     cnt.path = "Files", p = 0.5)
```
### Summarize results ATT_QU
```{r}
# For each quartile compute the median sumATT.
sumATT <- pisa18 %>%
  group_by(ATT_QU) %>%
  summarise(mATT = median(ATT), sd = sd(ATT),min = min(ATT),max = max(ATT))

medATT <- sumATT$mATT

dfATT_QU <- data.frame(x = rep(round(medATT,2),2),
                        group = c(rep("no",4),rep("yes",4)),
                        mean =  pintATT[,2],low = pintATT[,1], 
                        up = pintATT[,3])
sink(file = file.path("Results/dfATT_QU.txt"))
dfATT_QU
sink(file = NULL)
```


```{r}
pdf(file.path("Results/dfATT_QU.pdf"))
plot.result(dfATT_QU)
dev.off()
```

### Compute results RES_QU
```{r}
sgrRES <- split(pisa18,f = list(pisa18$RES_QU,pisa18$STAFFLACK))
#t<-lapply(sgrBL, function(x) length(x[[1]]))
pintRES <- comp.int.stack.core01(sgr = sgrRES,fits = fits,
                                     wts = wtsStacking,
                                     cnt.path = "Files", p = 0.5)
```
### Summarize results RES_QU
```{r}
# For each quartile compute the median sumRES.
sumRES <- pisa18 %>%
  group_by(RES_QU) %>%
  summarise(mRES = median(RES), sd = sd(RES),min = min(RES),max = max(RES))

medRES <- sumRES$mRES

dfRES_QU <- data.frame(x = rep(round(medRES,2),2),
                        group = c(rep("no",4),rep("yes",4)),
                        mean =  pintRES[,2],low = pintRES[,1], 
                        up = pintRES[,3])
sink(file = file.path("Results/dfRES_QU.txt"))
dfRES_QU
sink(file = NULL)
```

```{r}
pdf(file.path("Results/dfRES_QU.pdf"))
plot.result(dfRES_QU)
dev.off()
```
### DISHOME and three focal predictors
### BELONG
```{r}
sgrDHBL <- split(pisa18,f = list(pisa18$DH_QU,pisa18$BL_QU))
#t<-lapply(sgrDHBL, function(x) length(x[[1]]))
pintDHBL <- comp.int.stack.core01(sgr = sgrDHBL,fits = fits,
                                     wts = wtsStacking,
                                     cnt.path = "Files", p = 0.5)
```

### Summarize results DHBL
```{r}
dfDH_BL <- data.frame(x = rep(round(medDISHOME,2),2),
                        group = c(rep("low",4),rep("low.m",4),rep("up.m",4),rep("up",4)),
                        mean =  pintDHBL[,2],low = pintDHBL[,1], 
                        up = pintDHBL[,3])
sink(file = file.path("Results/dfDH_BL.txt"))
dfDH_BL
sink(file = NULL)
```

```{r}
pdf(file.path("Results/dfDH_BL.pdf"))
plot.result2(dfDH_BL)
dev.off()
```
### ATT
```{r}
sgrDHATT <- split(pisa18,f = list(pisa18$DH_QU,pisa18$ATT_QU))
#t<-lapply(sgrDHBL, function(x) length(x[[1]]))
pintDHATT <- comp.int.stack.core01(sgr = sgrDHATT,fits = fits,
                                     wts = wtsStacking,
                                     cnt.path = "Files", p = 0.5)
```
### Summarize results DHATT
```{r}
dfDH_ATT <- data.frame(x = rep(round(medDISHOME,2),2),
                        group = c(rep("low",4),rep("low.m",4),rep("up.m",4),rep("up",4)),
                        mean =  pintDHATT[,2],low = pintDHATT[,1], 
                        up = pintDHATT[,3])
sink(file = file.path("Results/dfDH_ATT.txt"))
dfDH_ATT
sink(file = NULL)
```

```{r}
pdf(file.path("Results/dfDH_ATT.pdf"))
plot.result2(dfDH_ATT)
dev.off()
```
### RES
```{r}
sgrDHRES <- split(pisa18,f = list(pisa18$DH_QU,pisa18$RES_QU))
#t<-lapply(sgrDHRES, function(x) length(x[[1]]))
pintDHRES <- comp.int.stack.core01(sgr = sgrRES,fits = fits,
                                     wts = wtsStacking,
                                     cnt.path = "Files", p = 0.5)
```

### Summarize results DHRES
```{r}
dfDH_RES <- data.frame(x = rep(round(medDISHOME,2),2),
                        group = c(rep("low",4),rep("low.m",4),rep("up.m",4),rep("up",4)),
                        mean =  pintDHRES[,2],low = pintDHRES[,1], 
                        up = pintDHRES[,3])
sink(file = file.path("Results/dfDH_RES.txt"))
dfDH_RES
sink(file = NULL)
```

```{r}
pdf(file.path("Results/dfDH_RES.pdf"))
plot.result2(dfDH_RES)
dev.off()
```
### RES
```{r}
pisa18$UNI_QU <- as.factor(cut_number(pisa18$UNI,4))
levels_UNI <- levels(pisa18$UNI_QU)
ds_UNI <- pisa18 %>% group_by(pisa18$UNI_QU) %>% summarise(median = median(pisa18$UNI), Freq = n())  
levels(pisa18$UNI_QU) <- 1:4

sgrDHUNI <- split(pisa18,f = list(pisa18$DH_QU,pisa18$UNI_QU))
#t<-lapply(sgrDHUNI, function(x) length(x[[1]]))
pintDHUNI <- comp.int.stack.core01(sgr = sgrDHUNI,fits = fits,
                                     wts = wtsStacking,
                                     cnt.path = "Files", p = 0.5)
```


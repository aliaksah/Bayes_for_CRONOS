#if (!require("haven")) install.packages("haven")
library("stringr")
library("dvmisc")
library("haven")
library("foreign")
library("ggplot2")
library("bayesrules")
library("ggeffects")
library("rstanarm")
library("broom")
library("bayesplot")
library("labelled")
library("HuraultMisc")
library("loo")
# Windows 10 users: loo may be very slow if 'mc.cores' is set in your .Rprofile file (see https://github.com/stan-dev/loo/issues/94).
library("LaplacesDemon")
library("xtable")
library("fabricatr")
library("scoring")
library("mice")
library("dplyr")
library("broom.mixed")
library("ggpubr")
library("patchwork")
library("kableExtra")
library("xtable")
library("DescTools")
library("gridExtra")
library("sjPlot")
library("see")
library("bayestestR")

### define root path to project
user <- Sys.info()["user"]
if (user == "zo95yup") {
  PPath <- "C:/Users/zo95yup/Documents/Dokumente/Forschung/EduPov"
} else if (user == "ba6st15") {
  PPath <- "C:/Users/ba6st15/Work Folders/My Files/Projekte/2022/edupov"
} else if (user == "Flo") {
  PPath <- "D:/Projekte/BMA"
}
# 0 Preamble ------------------------------------------------------------------

# folder structure
folders <- c("Orgdata", "Files", "Literature", "Output", "R")
for (i in folders) {
  if (!dir.exists(file.path(PPath,i))) dir.create(file.path(PPath,i))
}

# first time: --> copy original PISA 2018 files manually into "Orgdata"

source(file = file.path(PPath,"R", "00_Varlists.R"))

# 1 Data Import ---------------------------------------------------------------
# 
countries <- c("GRC", "NLD", "SWE", "IRL", "AUT", "PRT", "BGR", "DEU", "USA")

STUDENT <- read_spss(file.path(PPath, "Orgdata", "CY07_MSU_STU_QQQ.sav")) %>%
  filter(CNT %in% countries)

save("STUDENT",file = file.path(PPath, "Files","STUDENT_PISA.RData"))

SCHOOL <- read_spss(file.path(PPath, "Orgdata", "CY07_MSU_SCH_QQQ.sav")) %>%
  filter(CNT %in% countries)

save("SCHOOL",file = file.path(PPath, "Files","SCHOOL_PISA.RData"))


# 2 Data Preparation -----------------------------------------------------------

load(file.path(PPath, "Files","STUDENT_PISA.RData"))
load(file.path(PPath, "Files","SCHOOL_PISA.RData"))

# matching variable: CNTSCHID

intersect(names(SCHOOL), names(STUDENT))
DAT <- plyr::join(STUDENT, SCHOOL, by = "CNTSCHID")
dim(DAT)
save("DAT",file = file.path(PPath, "Files","DAT_PISA.RData"))

load(file = file.path(PPath, "Files","DAT_PISA.RData"))

source(file.path(PPath, "R", "00_varlists.R"))

source(file = file.path(PPath,"R", "01_Impute_Data.R"))

save("IMPDAT",file = file.path(PPath, "Files","IMPDAT_PISA.RData"))
load(file = file.path(PPath, "Files","IMPDAT_PISA.RData"))
# initial variable selection, data preparation
source(file = file.path(PPath,"R", "02_Prepare_Data.R"))

to.rm <- objects()[!objects() %in% c("PPath","pisa18","countries")]
rm(list = to.rm)
 
# 3 Data  Operationalization ----------------------------------------------
# There should be at least 10 students per school. Schools with fewer observations are excluded from the analysis.

tab.id <- table(pisa18$school.id)
id.frame <- as.data.frame(tab.id)

nb.stud <- rep(id.frame$Freq,id.frame$Freq)
pisa18$nb.stud <- nb.stud
rm(tab.id,id.frame,nb.stud)
pisa.data <- subset(pisa18, pisa18$nb.stud > 9)
pisa18 <- pisa.data

rm(pisa.data)
# Form the PISA scales for analysis.
source(file = file.path(PPath,"R", "03_Oper_Data.R"))
 
to.rm <- objects()[!objects() %in% c("PPath","pisa18","countries")]
rm(list = to.rm)

save(pisa18,file = file.path(PPath, "Files","pisa18.RData"))
load(file = file.path(PPath, "Files","pisa18.RData"))
table(pisa18$CNT)
# AUT  BGR  DEU  GRC  IRL  NLD  PRT  SWE  USA 
#6489 5215 5386 6228 5577 4759 5768 5376 4808 

## Steps
# model specification step
# estimation step
# ppc step
# predictive comparison  step

# 4 Model building ------------------------------------------------------------
#countries <- c("GRC", "NLD", "SWE", "IRL", "AUT", "PRT", "BGR", "DEU", "USA")
pisa18$SES <- as.factor(pisa18$ESCS4)
##### Step 1: Descriptive research focus
source(file = file.path(PPath,"R", "04_stack.R"))
#debugonce(load)
to.rm <- objects()[!objects() %in% c("PPath","gnau","bma.mod","full.ri.mod","modvarA",
                                    "full.mod","grc","prt","aut","usa","deu","swe","irl","nld","bgr",
                                    "countries5",
                                     "basic","basic01","models_01","models_01L3","models_01L4")]#"models",
rm(list = to.rm)
source(file = file.path(PPath,"R", "05_Functions.R"))

n01.models <- length(models_01)

# Step 2: Model specification: core and candidate models ->04_stack.R ########################################
# Step 3: Model estimation and  Bayesian stacking (with Brier Scores) ###########################################################
cnt.data <- aut
cnt.path <- file.path(PPath,"Final", "RES3")
a.seed <- 12345 
a.iter <-  2000 
a.chains <- 4  

# Information about the default priors: https://cran.r-project.org/web/packages/rstanarm/vignettes/priors.html
fits <- list()
#  Attitudes towards reading
fits[[1]] <- stan_glm(models_01[[1]], data = cnt.data, seed = a.seed,
                      iter = a.iter, chains = a.chains,family = binomial()) 
# Resources of students and school 
fits[[2]] <- stan_glm(models_01[[2]], data = cnt.data, seed = a.seed,
                      iter = a.iter, chains = a.chains,family = binomial()) 
# Attitudes towards school or learning
fits[[3]] <- stan_glm(models_01[[3]], data = cnt.data, seed = a.seed,
                      iter = a.iter, chains = a.chains,family = binomial())
# Relevant school context predictors
fits[[4]] <- stan_glm(models_01[[4]], data = cnt.data, seed = a.seed,
                      iter = a.iter, chains = a.chains,family = binomial())

save(fits, file = file.path(cnt.path,"fits.RData"))
#load(file.path(cnt.path,"fits.RData"))

core <- stan_glm(basic01, data = cnt.data, seed = a.seed,
                 iter = a.iter, chains = a.chains,family = binomial())
save(core, file = file.path(cnt.path,"core.RData"))
#load(file.path(cnt.path,"core.RData"))

# Write regression coefficients to a file.
sink(file = file.path(cnt.path,"fixed.txt"))
print(kable(tidy(core,conf.int = T, conf.level = .95, effects = "fixed"),"simple",digits = 4, 
            caption = paste("simple model")))
for (i in 1:n01.models) {
  print(kable(tidy(fits[[i]],conf.int  = T, conf.level = .95, 
                   effects = "fixed"),"simple",digits = 4, 
              caption = paste("model ",i," *****")))
}
sink(file = NULL)

sink(file = file.path(cnt.path,"fixedLaTex.txt"))
print(kable(tidy(core,conf.int = T, conf.level = .95, effects = "fixed"),"latex",
            digits = 4, caption = paste("core model")))
for (i in 1:n01.models) {
  print(kable(tidy(fits[[i]],conf.int = T, conf.level = .95, effects = "fixed"),
              "latex",digits = 4, caption = paste("model ",i," *****")))
}
sink(file = NULL)

# Convergence diagnostics
# https://mc-stan.org/rstan/reference/Rhat.html
# https://search.r-project.org/CRAN/refmans/bayestestR/html/diagnostic_posterior.html
# https://cran.r-project.org/web/packages/rstanarm/vignettes/rstanarm.html
# https://cran.r-project.org/web/packages/bayesplot/vignettes/plotting-mcmc-draws.html
# https://cran.r-project.org/web/packages/bayesplot/vignettes/visual-mcmc-diagnostics.html#effective-sample-size
# mcmc-diagnostics
#print(fits[[1]])
pdf(file.path(cnt.path,"mcmc-diagnostics.pdf"))
plot(fits[[1]], "trace")
#mcmc_acf(fits[[1]])
plot(fits[[2]], "trace")
#mcmc_acf(fits[[2]])
plot(fits[[3]], "trace")
#mcmc_acf(fits[[3]])
plot(fits[[4]], "trace")
#mcmc_acf(fits[[4]])
dev.off()

sink(file = file.path(cnt.path,"MCMCdiagnostic.txt"))
print("***************core*****************")
diagnostic_posterior(core)
neff_ratio(core)
print("**********************************************")
print("***************model 1*****************")
diagnostic_posterior(fits[[1]])
neff_ratio(fits[[1]])
print("**********************************************")
print("***************model 2*****************")
diagnostic_posterior(fits[[2]])
neff_ratio(fits[[2]])
print("**********************************************")
print("***************model 1*****************")
diagnostic_posterior(fits[[3]])
neff_ratio(fits[[3]])
print("**********************************************")
print("***************model 2*****************")
diagnostic_posterior(fits[[4]])
neff_ratio(fits[[4]])
print("**********************************************")
sink(file = NULL)


loo_list <- list()
# Compute loo

loo_list <- lapply(fits, loo, cores = 1)
# Save loo

save("loo_list", file = file.path(cnt.path,"loo_list.RData"))
# load(file.path(cnt.path,"loo_list.RData"))

sink(file = file.path(cnt.path,"loo_info.txt"))
for (i in 1:n01.models) {
  print(paste("******** "))
  print(paste("model ", i))
  print(loo_list[[i]])
  print(paste("******** "))
}
#print(loo_list[[1]])
sink(file = NULL)

wtsStacking <- list()

wtsStacking <- loo_model_weights(loo_list, method = "stacking")
print(wtsStacking)
# TODO -> names are lost?

sink(file = file.path(cnt.path,"wtsStacking.txt"))
print(wtsStacking)
print(paste("******** "))
sink(file = NULL)

# add core model to stack
# fits[[5]] <- core
# loo_list2 <- lapply(fits, loo, cores = 1)
# save("loo_list2", file = file.path(cnt.path,"loo_list2.RData"))

# wtsStacking2 <- list()

# wtsStacking2 <- loo_model_weights(loo_list2, method = "stacking")
# sink(file = file.path(cnt.path,"wtsStacking2.txt"))
# wtsStacking2
#print(paste("******** "))
#sink(file = NULL)

# draws from the posterior predictive distribution
ypredCore <- posterior_predict(core)
save("ypredCore", file = file.path(cnt.path,"ypredCore.RData"))
#dim(ypredCore), 4000 replicated data sets
#[1] 4000 6228
# load(file.path(cnt.path,"ypredCore.RData"))

ypredStack <- ypred_Stacking(fits,loo_list, method = "stacking") 
save("ypredStack", file = file.path(cnt.path,"ypredStack.RData"))
#load(file.path(cnt.path,"ypredStack.RData"))

ypredCand <- list()
for (i in 1:n01.models) {
  ypredCand[[i]] <- posterior_predict(fits[[i]])
}
save("ypredCand", file = file.path(cnt.path,"ypredCand.RData"))
# load(file.path(cnt.path,"ypredCand.RData"))


## Brier Score 
br.coreL2 <- brierS(cnt.data$LD,colMeans(ypredCore))
dim(ypredStack)
br.stackL2 <- brierS(cnt.data$LD,colMeans(ypredStack))

br.candL2 <- list()
for (i in 1:n01.models) {
  br.candL2[[i]] <- brierS(cnt.data$LD,colMeans(ypredCand[[i]]))
 
}

# Write Brier score to file.
sink(file = file.path(cnt.path,"brier.scores.txt"))
print("simple model *******")
print(br.coreL2)

print("stack *******")
print(br.stackL2)


for (i in 1:n01.models) {
  print(paste("model ", i))
  print(br.candL2[[i]])
  
}
sink(file = NULL)

# Step 4: Posterior predictive checks ###########################################################
# Used later to compute  the results
condPr1.data1 <- cnt.data %>%
  group_by(SES,DISHOME,FEM,RUR) %>% summarise(LD = sum(LD),
                                              FEMs = sum(FEM),MALES = sum(1 - FEM))
condPr1.total <- condPr1.data1$FEMs + condPr1.data1$MALES
condPr1.nLD <- condPr1.total - condPr1.data1$LD
condPr1.data2 <- cbind(condPr1.data1,condPr1.nLD,condPr1.total)

condPr1.data2$FEMs <- NULL
condPr1.data2$MALES <- NULL
condPr1.data <- condPr1.data2
names(condPr1.data) <- c("SES","DISHOME","FEM","RUR","LD","nDL","total")
# print(n=50,condPr1.data)

# define group-variable for group-specific posterior classifications
df_condPr <- data.frame(cnt.data$SES,cnt.data$DISHOME,cnt.data$FEM,cnt.data$RUR)
names(df_condPr) <- c("SES", "DISHOME","FEM" ,"RUR")
df_condPr$FESCS     <- with(df_condPr, interaction(SES,FEM), drop = TRUE)
df_condPr$AF     <- with(df_condPr, interaction(RUR,FEM), drop = TRUE)
df_condPr$AFA    <- with(df_condPr, interaction(AF,DISHOME), drop = TRUE)
cnt.data$FESCS  <- df_condPr$FESCS 
#table(cnt.data$condPr1)
table(cnt.data$FESCS)

# posterior classifications
predStack_01 <- as.numeric(colMeans(ypredStack) >= 0.5)
sensSt  <- rep(-1,8)
spezSt  <- rep(-1,8)
ovallA  <- rep(-1,8)

foc_groups <- levels(cnt.data$FESCS)
for (i in 1:8) {
  confusion_matrix <- table(cnt.data$LD[cnt.data$FESCS == foc_groups[i]],predStack_01[cnt.data$FESCS == foc_groups[i]])
  sensSt[i] <- confusion_matrix[2,2]/(confusion_matrix[2,2] + confusion_matrix[2,1])
  spezSt[i] <- confusion_matrix[1,1]/(confusion_matrix[1,2] + confusion_matrix[1,1])
  ovallA[i] <- (confusion_matrix[1,1] + confusion_matrix[2,2])/sum(confusion_matrix)
}

# For each quartile compute the median ESCS.
cnt.datacondPr1 <- cnt.data %>%
  group_by(SES) %>%
  summarise(mESCS = median(ESCS), sd = sd(ESCS),min = min(ESCS),max = max(ESCS))

meancondPr1 <- cnt.datacondPr1$mESCS

squcondPr1 <- split(cnt.data,f = list(cnt.data$SES,cnt.data$DISHOME,cnt.data$FEM,cnt.data$RUR))
# unlist(lapply(squcondPr1, function(x) length(x[[1]]))) not same order as 

prev8 <- cnt.data %>%
  group_by(FESCS) %>%
  summarise(mLD = mean(LD))


prev <- cnt.data %>%
  group_by(FEM,SES) %>% summarise(LD = mean(LD))

sink(file = file.path(cnt.path,"post_class.txt"))
print("Prev")
round(unlist(prev8$mLD),3)
print("Sens")
round(sensSt,3)
print("Spec")
round(spezSt,3)
print("overAll")
round(ovallA,3)
sink(file = NULL)
#  

# https://mc-stan.org/bayesplot/reference/available_ppc.html


# posterior checks


color_scheme_set(scheme = "purple")
pdf(file.path(cnt.path,"ppc_Core.pdf"))
visualPPC(y.obs = cnt.data$LD, yrepM = ypredCore)
dev.off()


color_scheme_set(scheme = "mix-teal-pink") # "brewer-Spectral"
pdf(file.path(cnt.path,"ppc_Stack.pdf"))
visualPPC(y.obs = cnt.data$LD, yrepM = ypredStack)
dev.off()


color_scheme_set(scheme = "red")
pdf(file.path(cnt.path,"ppc_Cand1.pdf"))
visualPPC(y.obs = cnt.data$LD, yrepM = ypredCand[[1]])
dev.off()

color_scheme_set(scheme = "blue")
pdf(file.path(cnt.path,"ppc_Cand2.pdf"))
visualPPC(y.obs = cnt.data$LD, yrepM = ypredCand[[2]])
dev.off()

color_scheme_set(scheme = "yellow")
pdf(file.path(cnt.path,"ppc_Cand3.pdf"))
visualPPC(y.obs = cnt.data$LD, yrepM = ypredCand[[3]])
dev.off()

color_scheme_set(scheme = "green")
pdf(file.path(cnt.path,"ppc_Cand4.pdf"))
visualPPC(y.obs = cnt.data$LD, yrepM = ypredCand[[4]])
dev.off()

save.image(file = file.path(cnt.path,"save_session.RData"))
#load(file=file.path(cnt.path,"save_session.RData"))

#### PPCBoxplot

ppc.data1a <- cnt.data %>%
  group_by(SES,FEM) %>% summarise(LD = sum(LD),FEMs = sum(FEM),MALEs = sum(1 - FEM))#
#  
head(ppc.data1a)
total <- ppc.data1a$FEMs + ppc.data1a$MALEs
nLD <- total - ppc.data1a$LD
ppc.data2a <- cbind(ppc.data1a,nLD,total)
#head(ppc.data2)
ppc.data2a$FEMs <- NULL
ppc.data2a$MALEs <- NULL
ppc.dataL2a <- ppc.data2a
names(ppc.dataL2a) <- c("SES","FEM","LD","nLD","total")

print(kable(ppc.dataL2a,"simple",digits = 4))


file.names <- "PPCboxStack.pdf"
ppcBoxplots(ypredStack,cnt.data,ppc.dataL2a,file.names)

# Step 5: Summarizing results based on the posterior distribution ##########################################
 
########

# 
#squcondPr1<-split(tcnt.data,f=list(tcnt.data$SES,tcnt.data$DISHOME,tcnt.data$FEM,tcnt.data$RUR))
#t<-unlist(lapply(squcondPr1, function(x) length(x[[1]])))
brE <- 4
pint_condPr1 <- comp.int.stack.core01(sgr = squcondPr1,fits = fits,
                                      wts = wtsStacking,cnt.path = cnt.path)
df_condPr1 <- data.frame(x = rep(round(meancondPr1,2),8),
               group = c(rep("adv. m. urb.",brE),rep("dis. m. urb.",brE),
                         rep("adv. f. urb.",brE),rep("dis. f. urb.",brE),
                         rep("adv. m. rur.",brE),rep("dis. m. rur.",brE),
                         rep("adv. f. rur.",brE),rep("dis. f. rur.",brE)),
                     mean =  pint_condPr1[,2],low = pint_condPr1[,1], 
                up = pint_condPr1[,3])
save(df_condPr1, file = file.path(cnt.path,"df_condPr1.RData"))

condPr1 <- cbind(as.data.frame(condPr1.data),as.data.frame(df_condPr1))
sink(file = file.path(cnt.path,"condPr1_interval.txt"))
condPr1
sink(file = NULL)

p_condPr1 <- plot.result(df_condPr1)

pdf(file.path(cnt.path,"plot_condPr1.pdf"))
p_condPr1
dev.off()

save.image(file = file.path(cnt.path,"save_session.RData"))
#load(file=file.path(cnt.path,"save_session.RData"))


#### focal gap  #################

# squcondPr1  <-split(cnt.data,f=list(cnt.data$SES,cnt.data$DISHOME,cnt.data$FEM,cnt.data$RUR))
squcondPr1_gap <- split(cnt.data,f = list(cnt.data$SES,cnt.data$DISHOME,cnt.data$RUR,cnt.data$FEM))
#t<-lapply(squcondPr1_gap, function(x) length(x[[1]]))
#draws from the post.-pred. distribution for 32 groups -> buid 16 differences 
pint_condPr1_gap.inner <- comp.int.stack.core01.inner(sgr = squcondPr1_gap,
                                                      fits = fits,
                                                      wts = wtsStacking,
                                                      cnt.path = cnt.path)
l <- length(pint_condPr1_gap.inner)
#t<-cbind(1:32,lapply(squcondPr1_gap, function(x) length(x[[1]])))
pint_gap.m <-  matrix(unlist(pint_condPr1_gap.inner), nrow = l, byrow = TRUE)
#dim(pint_gap.m)
#pint_condPr1_gap.inner[[1]]
#pint_gap.m[1,]



pint_gap <- pint_gap.m[1:16,] - pint_gap.m[17:32,]
conf.pred <- list()

for (i in 1:16) {
  conf.pred[[i]] <- quantile(pint_gap[i,], probs = c(0.25,0.5,0.75))
}
pint_gap <- do.call(rbind, conf.pred)

brA <- 4
df_gap <- data.frame(x = rep(round(meancondPr1,2),4),
               #     group = c(rep("ar",brA),rep("dr",brA),rep("au",brA),rep("du",brA) ),
               group = c(rep("adv. urb.",brA),rep("dis. urb.",brA),
                         rep("adv. rur.",brA),rep("dis. rur.",brA) ),
                    mean =  pint_gap[,2],low = pint_gap[,1], up = pint_gap[,3])
save(df_gap, file = file.path(cnt.path,"df_condPr.RData"))

sink(file = file.path(cnt.path,"gap_interval.txt"))
df_gap
sink(file = NULL)

p_gap <- plot.resultGAP(df_gap)

pdf(file.path(cnt.path,"plot_gapGen.pdf"))
p_gap
dev.off()

save.image(file = file.path(cnt.path,"save_session.RData"))
# load(file.path(cnt.path,"save_session.RData"))